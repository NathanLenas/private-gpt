services:
  private-gpt-gpu:
    build:
      dockerfile: Dockerfile.external.gpu
    volumes:
      - ./local_data/:/home/worker/app/local_data
      - ./models/:/home/worker/app/models
      - venv:/home/worker/app/.venv
    ports:
      - 8002:8080 
    environment:
      PORT: 8080
      PGPT_PROFILES: docker
      PGPT_LLM_MODE: ollama
      PGPT_OLLAMA_LLM_MODEL: ${PGPT_OLLAMA_LLM_MODEL:-llama3}
      PGPT_LLM_TOKENIZER: ${PGPT_LLM_TOKENIZER:-meta-llama/Meta-Llama-3-8B-Instruct}
      PGPT_OLLAMA_EMBEDDING_MODEL: ${PGPT_OLLAMA_EMBEDDING_MODEL:-mxbai-embed-large}
      PGPT_OLLAMA_TFS_Z: ${PGPT_OLLAMA_TFS_Z:-1.0}
      PGPT_OLLAMA_TOP_K: ${PGPT_OLLAMA_TOP_K:-40}
      PGPT_OLLAMA_TOP_P: ${PGPT_OLLAMA_TOP_P:-0.9}
      PGPT_OLLAMA_REPEAT_LAST_N: ${PGPT_OLLAMA_REPEAT_LAST_N:-64}
      PGPT_OLLAMA_REPEAT_PENALTY: ${PGPT_OLLAMA_REPEAT_PENALTY:-1.2}
      PGPT_RAG_TOP_K: ${PGPT_RAG_TOP_K:-3}
      PGPT_RAG_RERANK: ${PGPT_RAG_RERANK:-false}
      PGPT_RAG_RERANK_MODEL: ${PGPT_RAG_RERANK_MODEL:-cross-encoder/ms-marco-MiniLM-L-2-v2}
      PGPT_RAG_RERANK_TOP_N: ${PGPT_RAG_RERANK_TOP_N:-4}
      PGPT_EMBED_DIM: ${PGPT_EMBED_DIM:-512}
      PGPT_OLLAMA_MAX_NEW_TOKENS: ${PGPT_OLLAMA_MAX_NEW_TOKENS:-512}
      PGPT_OLLAMA_CONTEXT_WINDOW: ${PGPT_OLLAMA_CONTEXT_WINDOW:-3900}
      HF_TOKEN: ${HF_TOKEN:-}
      RUN_CMD: ${RUN_CMD:-wiperun}
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
              
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11435:11435"
    environment:
      - OLLAMA_HOST=0.0.0.0:11435
    volumes:
      - ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
    

    
volumes:
 ollama:
 venv:
