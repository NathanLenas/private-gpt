services:

  #-----------------------------------
  #---- Private-GPT services ---------
  #-----------------------------------

  # Private-GPT service for the Ollama CPU and GPU modes
  # This service builds from an external Dockerfile and runs the Ollama mode.
  private-gpt-ollama:
    image: ${PGPT_IMAGE:-zylonai/private-gpt}${PGPT_TAG:-0.6.1}-ollama
    build:
      context: .
      dockerfile: Dockerfile.ollama
    volumes:
      - ./local_data/:/home/worker/app/local_data
    ports:
      - "8001:8001"
    environment:
      PORT: 8001
      PGPT_PROFILES: docker
      PGPT_MODE: ollama
      PGPT_EMBED_MODE: ollama
      PGPT_OLLAMA_API_BASE: http://ollama:11434
      HF_TOKEN: ${HF_TOKEN:-}
      
      PGPT_OLLAMA_LLM_MODEL: ${PGPT_OLLAMA_LLM_MODEL:-llama3.1}
      PGPT_LLM_TOKENIZER: ${PGPT_LLM_TOKENIZER:-}
      PGPT_OLLAMA_EMBEDDING_MODEL: ${PGPT_OLLAMA_EMBEDDING_MODEL:-}
      PGPT_EMBED_DIM: ${PGPT_EMBED_DIM:-}
      PGPT_OLLAMA_TEMPERATURE: ${PGPT_OLLAMA_TEMPERATURE:-}
      PGPT_OLLAMA_TFS_Z: ${PGPT_OLLAMA_TFS_Z:-}
      PGPT_OLLAMA_TOP_K: ${PGPT_OLLAMA_TOP_K:-}
      PGPT_OLLAMA_TOP_P: ${PGPT_OLLAMA_TOP_P:-}
      PGPT_OLLAMA_REPEAT_LAST_N: ${PGPT_OLLAMA_REPEAT_LAST_N:-}
      PGPT_OLLAMA_REPEAT_PENALTY: ${PGPT_OLLAMA_REPEAT_PENALTY:-}
      PGPT_OLLAMA_MAX_NEW_TOKENS: ${PGPT_OLLAMA_MAX_NEW_TOKENS:-}
      PGPT_OLLAMA_CONTEXT_WINDOW: ${PGPT_OLLAMA_CONTEXT_WINDOW:-}
      PGPT_RAG_SIMILARITY_TOP_K: ${PGPT_RAG_SIMILARITY_TOP_K:-}
      PGPT_RAG_RERANK: ${PGPT_RAG_RERANK:-}
      PGPT_RAG_RERANK_MODEL: ${PGPT_RAG_RERANK_MODEL:-}
      PGPT_RAG_RERANK_TOP_N: ${PGPT_RAG_RERANK_TOP_N:-}

      
      


    profiles:
      - ""
      - ollama-cpu
      - ollama-cuda
      - ollama-api

  # Private-GPT service for the local mode
  # This service builds from a local Dockerfile and runs the application in local mode.
  private-gpt-llamacpp-cpu:
    image: ${PGPT_IMAGE:-zylonai/private-gpt}${PGPT_TAG:-0.6.1}-llamacpp-cpu
    build:
      context: .
      dockerfile: Dockerfile.llamacpp-cpu
    volumes:
      - ./local_data/:/home/worker/app/local_data
      - ./models/:/home/worker/app/models
    entrypoint: sh -c ".venv/bin/python scripts/setup && .venv/bin/python -m private_gpt"
    ports:
      - "8001:8001"
    environment:
      PORT: 8001
      PGPT_PROFILES: local
      HF_TOKEN: ${HF_TOKEN}
    profiles:
      - llamacpp-cpu

  #-----------------------------------
  #---- Ollama services --------------
  #-----------------------------------

  # Ollama service for the CUDA mode
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ./models:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - ollama-cuda

  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    container_name: qdrant
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
    configs:
      - source: qdrant_config
        target: /qdrant/config/production.yaml
    volumes:
      - ./local_data/qdrant_data/:/qdrant/storage
    
configs:
  qdrant_config:
    content: |
      log_level: INFO  
